# Elixir Days - Nx + LiveView

```elixir
Mix.install([
  :req
])
```

## Introdução

Nesta palestra, veremos um passo a passo de como adicionar uma funcionalidade de Inteligência Artificial em uma aplicação Phoenix LiveView.

No caminho, discutiremos algumas decisões de arquitetura e de comunicação entre cliente e servidor.

Como exemplo, vamos utilizar um modelo de classificação de imagens com Bumblebee.

## Preparação

1. Criar o projeto: `mix phx.new elixir_days`
2. Adicionar as dependências:

<!-- livebook:{"force_markdown":true} -->

```elixir
def deps do
  [
    # ...,
    {:stb_image, "~> 0.6.8"},
    {:bumblebee, "~> 0.5"},
    {:exla, "~> 0.7.2"} # requer Make e gcc
  ]
end
```

1. Adicionar configurações do `:nx` no `runtime.exs`

<!-- livebook:{"force_markdown":true} -->

```elixir
config :nx, :default_backend, EXLA.Backend
config :nx, :default_defn_options, [compiler: EXLA]
```

## Criando a LiveView

Vamos introduzir uma nova página em `/image_classification`.

A página vai capturar imagens da webcam, enviá-las para o servidor, e retornar uma classificação de qual objeto está aparecendo.

```elixir
# elixir_days_web/live/image_classification_live.ex
defmodule ElixirDaysWeb.ImageClassificationLive do
  use ElixirDaysWeb, :live_view

  def mount(_params, _session, socket) do
    {:ok,
     assign(socket,
       frames: [],
       predictions: %{},
       total_bytes: 0,
       total_seconds: 0,
       start_time: System.monotonic_time(:second)
     )}
  end

  def render(assigns) do
    ~H"""
    <div id="webcam-container" phx-hook="WebcamHook">
      <video id="webcam" width="640" height="480" autoplay></video>
    </div>
    <div class="py-4" style="width: 640px">
      <%= for prediction <- Enum.sort_by(@predictions, & &1.score, :desc) do %>
        <div class="w-full text-gray-800 text-sm font-semibold mr-2 mb-2">
          <div class="inline-block w-1/6" />
          <div class={"inline-block py-2 px-4 w-1/3 rounded-l-full #{if(prediction.confident?, do: "bg-green-500", else: "bg-gray-200")}"}>
            <%= prediction.label %>
          </div>
          <div class={"inline-block py-2 px-4 w-1/3 rounded-r-full text-center #{if(prediction.confident?, do: "bg-green-500", else: "bg-gray-200")}"}>
            <%= prediction.score %>
          </div>
        </div>
      <% end %>
    </div>
    <div class="">
      <p class="text-sm text-gray-800">
        Total bytes: <%= "#{:erlang.float_to_binary(@total_bytes / 1_000_000, decimals: 2)} MB" %>
      </p>
      <p class="text-sm text-gray-800">Total seconds: <%= @total_seconds %></p>
      <p class="text-sm text-gray-800">
        Average Data Rate: <%= if(@total_seconds == 0,
          do: "0",
          else: :erlang.float_to_binary(@total_bytes / (@total_seconds * 1_000), decimals: 2)
        ) <> " KB/s" %>
      </p>
    </div>
    """
  end

  def handle_event("frame", %{"frame" => frame_data}, socket) do
    # Process the frame data here (e.g., store or analyze)
    image = StbImage.read_binary!(Base.decode64!(frame_data))
    %{predictions: classification} = Nx.Serving.batched_run(ImageClassifierServing, image)

    classification =
      Enum.map(classification, fn %{score: score, label: label} ->
        if score > 0.1 do
          score = trunc(score * 10 ** 3)
          [label | _] = String.split(label, ",", parts: 2)
          %{confident?: score > 0.5, score: "#{div(score, 10)}.#{rem(score, 10)}%", label: label}
        else
          %{confident?: false, score: "-", label: "-"}
        end
      end)

    {:noreply,
     assign(socket,
       predictions: classification,
       total_bytes: socket.assigns.total_bytes + byte_size(frame_data),
       total_seconds: System.monotonic_time(:second) - socket.assigns.start_time
     )}
  end
end
```

```elixir
# elixir_days_web/router.ex

# ...
scope "/", ElixirDaysWeb do
  pipe_through(:browser)

  # ...
  live("/image_classification", ImageClassificationLive)
end

# ...
```

`assets/js/webcam_hook.js`

```javascript
export const WebcamHookMount = (hook) => {
  const fps = 1;
  const interval = 1000 / fps;

  function captureFrame(video) {
    const canvas = document.createElement("canvas");
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    const context = canvas.getContext("2d");
    context.drawImage(video, 0, 0, canvas.width, canvas.height);
    const dataUrl = canvas.toDataURL("image/png");
    const frameData = dataUrl.replace(/^data:image\/(png|jpeg);base64,/, "");

    // Push the frame to LiveView
    hook.pushEvent("frame", { frame: frameData });
  }

  const video = document.getElementById("webcam");
  if (navigator.mediaDevices.getUserMedia) {
    navigator.mediaDevices
      .getUserMedia({ video: true })
      .then(function (stream) {
        video.srcObject = stream;
        setInterval(captureFrame, interval, video);
      })
      .catch(function (error) {
        console.log("Something went wrong!");
      });
  }
};
```

<!-- livebook:{"break_markdown":true} -->

`assets/js/app.js`

```script
...

import { WebcamHookMount } from "./webcam_hook";

WebcamHook = {
  mounted() {
    WebcamHookMount(this);
  },
};

...

let liveSocket = new LiveSocket("/live", Socket, {
  longPollFallbackMs: 2500,
  params: { _csrf_token: csrfToken },
  hooks: { WebcamHook },
});

...
```

## Otimizando a transmissão de dados

Podemos adicionar mais processamento no lado do cliente, e um pouco do lado do servidor para reduzir a quantidade de dados trafegados.

<!-- livebook:{"break_markdown":true} -->

Em vez de:

```javascript
const dataUrl = canvas.toDataURL("image/png");
const frameData = dataUrl.replace(/^data:image\/(png|jpeg);base64,/, "");
hook.pushEvent("frame", { frame: frameData });
```

Podemos usar:

```javascript
import pako from "pako";

...


// Create a second canvas for resizing
const resizedCanvas = document.createElement("canvas");
const resizedContext = resizedCanvas.getContext("2d");

// Draw the original canvas onto the resized canvas
resizedContext.drawImage(
  canvas,
  0,
  0,
  canvas.width,
  canvas.height,
  0,
  0,
  244,
  244
);

const dataUrl = resizedCanvas.toDataURL("image/png");
const frameData = dataUrl.replace(/^data:image\/(png|jpeg);base64,/, "");

// Convert the base64 string to a binary string
const binaryString = atob(frameData);

// Convert the binary string to a byte array
const byteArray = new Uint8Array(binaryString.length);
for (let i = 0; i < binaryString.length; i++) {
  byteArray[i] = binaryString.charCodeAt(i);
}

// Compress the byte array using pako
const compressedData = pako.deflate(byteArray);

// Convert the compressed byte array to a base64 string
const compressedBase64 = btoa(
  Array.from(compressedData)
    .map((char) => String.fromCharCode(char))
    .join("")
);

// Push the frame to LiveView
hook.pushEvent("frame", { frame: compressedBase64 });
```

Este código reduz o tamanho da imagem para o tamanho efetivo que o modelo ResNet utilizará, e depois utiliza a biblioteca `pako` para comprimir os dados.

Para decodificar do lado do servidor, em vez de:

<!-- livebook:{"force_markdown":true} -->

```elixir
frame_data = Base.decode64!(frame_data)
image = StbImage.read_binary!(frame_data)
```

usaremos:

<!-- livebook:{"force_markdown":true} -->

```elixir
frame_data = Base.decode64!(frame_data)

z = :zlib.open()
:ok = :zlib.inflateInit(z)
decompressed_data = :zlib.inflate(z, frame_data)
:ok = :zlib.inflateEnd(z)
:zlib.close(z)

decompressed_data = IO.iodata_to_binary(decompressed_data)
image = StbImage.read_binary!(decompressed_data)
```

<!-- livebook:{"break_markdown":true} -->

Com isso, conseguimos uma redução de ~500KB/s para ~85KB/s de banda, sem perdas nos resultados.
